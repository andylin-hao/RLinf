RL Training with World Model Simulator
======================================

.. |huggingface| image:: /_static/svg/hf-logo.svg
   :width: 16px
   :height: 16px
   :class: inline-icon

This document provides a comprehensive guide to launching and managing the 
Vision-Language-Action Models (VLAs) training task within the RLinf framework,
using the **Action-conditioned Opensora World Model Simulator** (hereafter referred to as Opensora)
as the environment backend.

The primary objective is to train the policy in a closed-loop fashion without requiring real robots 
or traditional physics simulators, by leveraging a visual generation model to simulate 
how the environment evolves in response to actions.

Similar to finetuning VLAs in the LIBERO environment, this guide focuses on how to run 
reinforcement learning training tasks in the Opensora-based simulation environment, 
highlighting the key capabilities of the model within this framework.

Opensora aims to endow the model with the following capabilities:

1. **Visual Understanding**: Opensora generates future video frames from current observations and given action sequences, providing continuous visual feedback to the policy, enabling it to process RGB images from real robot cameras.
2. **Language Comprehension**: Understanding natural-language task descriptions.
3. **Action Generation**: Producing precise robotic actions (position, rotation, gripper control).
4. **Policy Improvement**: Leveraging "imagined" trajectories generated by Opensora to optimize the VLA policy using reinforcement learning methods such as PPO.

Similar to the VLA finetuning workflow in the LIBERO environment, this document focuses on how to run RL training tasks in the Opensora-based simulation environment.

Environment
-----------------------

As a world model, Opensora can theoretically fit any environment for any task while maintaining a consistent interface. Using the **LIBERO environment** as an example, the environment interfaces and definitions are as follows:

**Opensora Simulating LIBERO Environment**

- **Environment**: Visual generation model
- **Task**: Command a 7-DoF robotic arm to perform a variety of household manipulation skills (pick-and-place, stacking, opening drawers, spatial rearrangement)
- **Observation**: Images returned by the visual generation model
- **Action Space**: 7-dimensional continuous actions  
  - 3D end-effector position control (x, y, z)  
  - 3D rotation control (roll, pitch, yaw)  
  - Gripper control (open / close)

**Opensora Simulating LIBERO Environment Reset**

Unlike traditional simulators that can reset directly via reset(), Opensora requires initialization frames and task descriptions for initialization and reset. Therefore, we need to download the corresponding initialization dataset in advance and specify the path to the initialization dataset.

**Data Structure**

- **Images**: RGB tensors ``[batch_size, 256, 256, 3]``
- **Task Descriptions**: Natural-language instructions
- **Actions**: Normalized continuous values converted to discrete tokens
- **Rewards**: Provided by the reward classifier in the world model, ranging from 0 to 1

Algorithm
-----------------------------------------

**Core Algorithm Components**

1. **PPO (Proximal Policy Optimization)**

   - Advantage estimation using GAE (Generalized Advantage Estimation)
   - Policy clipping with ratio limits
   - Value function clipping
   - Entropy regularization

2. **GRPO (Group Relative Policy Optimization)**

   - For every state / prompt, the policy generates *G* independent actions
   - Compute the advantage of each action by subtracting the group's mean reward

3. **Vision-Language-Action Model**

   - OpenVLA architecture with multimodal fusion
   - Action tokenization and de-tokenization
   - Value head for critic function


Dependency Installation
-----------------------

1. Clone RLinf Repository
~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code:: bash

   # For mainland China users, you can use the following for better download speed:
   # git clone https://ghfast.top/github.com/RLinf/RLinf.git
   git clone https://github.com/RLinf/RLinf.git
   cd RLinf

2. Install Dependencies
~~~~~~~~~~~~~~~~~~~~~~~~~~

**Option 1: Docker Image**

Use Docker image for the experiment.

.. code:: bash

   docker run -it --rm --gpus all \
      --shm-size 20g \
      --network host \
      --name rlinf \
      -v .:/workspace/RLinf \
      rlinf/rlinf:agentic-rlinf0.1-torch2.6.0-openvla-openvlaoft-pi0
      # For mainland China users, you can use the following for better download speed:
      # docker.1ms.run/rlinf/rlinf:agentic-rlinf0.1-torch2.6.0-openvla-openvlaoft-pi0

Please switch to the corresponding virtual environment via the built-in `switch_env` utility in the image:

.. code:: bash

   source switch_env openvla-oft

**Option 2: Custom Environment**

.. code:: bash

   # For mainland China users, you can add the `--use-mirror` flag to the install.sh command for better download speed.
   # First, install the dependencies for the algorithm (openvla-oft) and simulation environment (maniskill_libero)
   bash requirements/install.sh embodied --model openvla-oft --env maniskill_libero
   # Then, install the opensora dependencies
   bash requirements/install.sh embodied --model opensora --env maniskill_libero
   source .venv/bin/activate

VLA Model Download
------------------

Before starting training, you need to download the corresponding pretrained model:

.. code:: bash

   # Download the model (choose either method)
   # Method 1: Using git clone
   git lfs install
   git clone https://huggingface.co/RLinf/RLinf-OpenVLAOFT-LIBERO-90-Base-Lora
   git clone https://huggingface.co/RLinf/RLinf-OpenVLAOFT-LIBERO-130-Base-Lora

   # Method 2: Using huggingface-hub
   # For mainland China users, you can use the following for better download speed:
   # export HF_ENDPOINT=https://hf-mirror.com
   pip install huggingface-hub
   hf download RLinf/RLinf-OpenVLAOFT-LIBERO-90-Base-Lora --local-dir RLinf-OpenVLAOFT-LIBERO-90-Base-Lora
   hf download RLinf/RLinf-OpenVLAOFT-LIBERO-130-Base-Lora --local-dir RLinf-OpenVLAOFT-LIBERO-130-Base-Lora

After downloading, make sure to correctly specify the model path in the configuration yaml file.

.. code:: yaml

   rollout:
      model:
         model_path: Pathto/RLinf/RLinf-OpenVLAOFT-LIBERO-90-Base-Lora
   actor:
      model:
         model_path: Pathto/RLinf/RLinf-OpenVLAOFT-LIBERO-90-Base-Lora

WM (World Model) Model Download
--------------------------------

In addition to the VLA model, you need to download the Opensora weights and the dataset for simulation initialization. Currently, RLinf only provides weights and data for libero-spatial and libero-object. The download methods are as follows:

.. code:: bash

   # Download weights and data
   # Method 1: Using git clone
   git lfs install
   git clone https://huggingface.co/jzndd/Opensora_for_libero

   # Method 2: Using huggingface-hub
   pip install huggingface-hub
   hf download jzndd/Opensora_for_libero

The directory structure of Opensora_for_libero is as follows:

.. code-block:: text

    Opensora_for_libero/
    └── libero_spatial/  (or libero_object)
        ├── best_wm_ckpt/
        │   └── base_policy/
        │       ├── model/                      # World model weight files
        │       └── dataset_statistics.json     # Dataset normalization statistics
        ├── best_rm_ckpt/
        │   └── resnet_rm.pth                   # Reward model weight file
        └── dataset_for_rlinf_world_model_init/ # Simulation initialization dataset
            └── base_policy_rollout_buffer/
                ├── traj0.npy                   # Initial state frame data
                ├── traj1.npy
                ├── ...
                └── trajN.npy
        └── vae/                                # VAE model weight files

After downloading, make sure to correctly specify the model path in the configuration yaml file.

.. code:: yaml

    env:
        train:
            opensora_wm_hf_ckpt_path: /Pathto/dataset/Opensora_for_libero/

Running the Script
-------------------

**1. Key Parameters Configuration**

.. code-block:: yaml

   cluster:
      num_nodes: 2
      component_placement:
         env: 0-7
         rollout: 8-15
         actor: 0-15

   rollout:
      pipeline_stage_num: 2

Here you can flexibly configure the GPU count for env, rollout, and actor components.
Additionally, by setting `pipeline_stage_num = 2` in the configuration, you can achieve **pipeline overlap between rollout and env**, improving rollout efficiency.

.. code-block:: yaml
   
   cluster:
      num_nodes: 1
      component_placement:
         env,rollout,actor: all

You can also reconfigure the placement to achieve **complete sharing**, where env, rollout, and actor components all share all GPUs.

.. code-block:: yaml

   cluster:
      num_nodes: 2
      component_placement:
         env: 0-3
         rollout: 4-7
         actor: 8-15

You can also reconfigure the placement to achieve **complete separation**, where env, rollout, and actor components each use their own GPUs without interference, eliminating the need for offload functionality.

**2. Configuration Files**

We support the **OpenVLA-OFT** model with the **GRPO** algorithm.  
The corresponding configuration file is:

- **OpenVLA-OFT + GRPO**: ``examples/embodiment/config/opensora_libero_spatial_grpo_openvlaoft.yaml``

**3. Launch Commands**

To start training with a chosen configuration, run the following command:

.. code-block:: bash

   bash examples/embodiment/run_embodiment.sh CHOSEN_CONFIG

For example, to use Opensora to simulate the libero-spatial environment and train the OpenVLA-OFT model using the GRPO algorithm, run:

.. code-block:: bash

   bash examples/embodiment/run_embodiment.sh opensora_libero_spatial_grpo_openvlaoft

Visualization and Results
-------------------------

**1. TensorBoard Logging**

.. code-block:: bash

   # Start TensorBoard
   tensorboard --logdir ./logs --port 6006

**2. Key Metrics Tracked**

- **Training Metrics**:

  - ``train/actor/approx_kl``: Approximate KL divergence to monitor policy update magnitude.
  - ``train/actor/clip_fraction``: Fraction of updates where the probability ratio triggered PPO clipping.
  - ``train/actor/clipped_ratio``: Mean of the clipped probability ratios, measuring how much policy updates are affected by clipping.
  - ``train/actor/grad_norm``: Gradient norm.
  - ``train/actor/lr``: Learning rate.
  - ``train/actor/policy_loss``: PPO/GRPO policy loss.
  - ``train/critic/value_loss``: Value function loss.
  - ``train/critic/value_clip_ratio``: Fraction of value targets whose update was clipped in PPO-style value function clipping.
  - ``train/critic/explained_variance``: Explained variance of value function predictions; closer to 1 is better.
  - ``train/entropy_loss``: Policy entropy.
  - ``train/loss``: Total training loss (actor_loss + critic_loss + entropy_loss regularization).

- **Rollout Metrics**:

  - ``rollout/advantages_max``: Maximum of the advantage function.
  - ``rollout/advantages_mean``: Mean of the advantage function.
  - ``rollout/advantages_min``: Minimum of the advantage function.
  - ``rollout/rewards``: Chunk of reward (refer to L414 in libero_env.py).

- **Environment Metrics**:

  - ``env/episode_len``: Number of environment steps elapsed in the episode (unit: step).
  - ``env/return``: Episode return. In LIBERO's sparse-reward setting, this metric is not informative since the reward is almost always 0 until the terminal success step.
  - ``env/reward``: Step-level reward (0 for all intermediate steps and 1 only at successful termination).  
    The logged value is normalized by the number of episode steps, which makes it difficult to interpret as real task performance during training.
  - ``env/success_once``: Recommended metric to monitor training performance. It directly reflects the unnormalized episodic success rate and better represents the true performance of the policy.

**3. Video Generation**

.. code-block:: yaml

   env:
      eval:
         video_cfg:
            save_video: True
            video_base_dir: ${runner.logger.log_path}/video/eval

**4. Train Log Tool Integration**

.. code-block:: yaml

   runner:
      task_type: embodied
      logger:
         log_path: "../results"
         project_name: rlinf
         experiment_name: "libero_10_grpo_openvlaoft"
         logger_backends: ["tensorboard"] # wandb, swanlab

LIBERO Partial Results
~~~~~~~~~~~~~~~~~~~~~~

Currently, we have only tested using Opensora to simulate libero-spatial and libero-object environments and trained VLA models. More environments are still under testing.

For each LIBERO suite, we evaluate every combination of task_id and trial_id. For the Object and Spatial suites, we evaluate 500 environments in total (10 tasks × 50 trials).

We evaluate each model according to its training configuration:
For the SFT-trained (LoRA-base) models, we set `do_sample = False`.
For the RL-trained models, we set `do_sample = True`, `temperature = 1.6`, and enable `rollout_epoch=2` to elicit the best performance of the RL-tuned policy.

.. note:: 
    
    The motivation for choosing Opensora as a world model simulator comes from `WMPO <https://arxiv.org/abs/2511.09515>`_. In the actual training of the world model, we referred to `WMPO <https://arxiv.org/abs/2511.09515>`_ and `Opensora <https://github.com/jzndd/opensora>`_.

.. list-table:: **Evaluation results on LIBERO task groups using Opensora simulator**
    :header-rows: 1
    :widths: 50 25 25

    * - Model
      - Object
      - Spatial
    * - |huggingface| `OpenVLA-OFT (LoRA-base) <https://huggingface.co/RLinf/RLinf-OpenVLAOFT-LIBERO-130-Base-Lora>`_
      - 50.20%
      - 51.61%
    * - OpenVLA-OFT (RLinf-GRPO with Opensora as world model simulator)
      - 75.5%
      - 64.5%
    * - **Improvement**
      - **+25.3%**
      - **+12.9%**

