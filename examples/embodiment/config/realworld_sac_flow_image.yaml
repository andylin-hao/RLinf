defaults:
  - env/realworld_peg_insertion@env.train
  - env/realworld_peg_insertion@env.eval
  - model/flow_policy@actor.model # USE flow_policy.yaml
  - training_backend/fsdp@actor.fsdp_config
  - override hydra/job_logging: stdout

hydra:
  run:
    dir: .
  output_subdir: null
  searchpath:
    - file://${oc.env:EMBODIED_PATH}/config/

cluster:
  num_nodes: 2
  component_placement:
    actor: 
      node_group: "4090"
      placement: 0 # Local GPU rank in the 4090 node group (n for the n+1 th GPU)
    env:
      node_group: franka
      placement: 0 # Local robot rank in the franka node group (n for the n+1 th robot)
    rollout:
      node_group: "4090"
      placement: 0 # Local GPU rank in the 4090 node group (n for the n+1 th GPU)
  node_groups:
    - label: "4090" # GPU node group
      node_ranks: 0 # The ranks of the nodes to be included in this group， change this if your training node is not rank 0
    - label: franka
      node_ranks: 1 # Change this if your robot node is not rank 1
      hardware:
        type: Franka
        configs:
          - robot_ip: 10.100.25.107 # Replace with your robot's IP address
            camera_serials: ["233622071355"]  # 单D435i  
            # camera_serials: ["409122273937", "409122274720"] # 双D405 顺序和collect_data相同
            # camera_serials: ["409122273937"]  # 单D405测试
            node_rank: 1
            disable_validate: True # hongzhi added

data:
  type: robot_demo
  channel:
    name: demo_data
  # path: "/home/franka/shengyz/RLinf/data_12312218.pkl"
  # path: "/home/franka/shengyz/RLinf/data_01031105.pkl" # [修复 replay_buffer/size ] 0103
  # path: "/home/franka/shengyz/RLinf/data_01041714.pkl" # 完整测试最新commit
  # path: "/home/franka/shengyz/RLinf/data_01052140.pkl" # 0105 测试双D405
  # path: "/home/franka/shengyz/RLinf/data_01062255.pkl" # 0106 换board和peg，再测试双D405 (机械臂抽风后恢复)
  path: "/home/franka/shengyz/RLinf/data_01071754.pkl" # 0107 最普通单相机能看到peg太少，改俯视支架

runner:
  task_type: embodied
  logger:
    log_path: "../results"
    project_name: rlinf
    experiment_name: "sac_flow"
    logger_backends: ["tensorboard"] # wandb, swanlab

  max_epochs: 8000 # r0: 16000 -> 8000
  max_steps: -1

  only_eval: False
  val_check_interval: -1  # r1: 100 -> -1 as in realworld_peginsertion_rlpd_cnn_async.yaml
  save_interval: -1

algorithm:
  update_epoch: 30  # r2: 32 -> 30
  group_size: 1
  auto_reset: True
  ignore_terminations: True
  use_fixed_reset_state_ids: False
  require_values: False  # SAC doesn't use value function for advantages
  normalize_advantages: False  # SAC doesn't use advantages
  obs_mode: rgb
  kl_penalty: kl
  
  agg_q: mean # r3: min -> mean

  backup_entropy: False  # r7: True -> False
  critic_subsample_size: 2 # r: added
  eval_rollout_epoch: 1

  n_chunk_steps: 2
  n_eval_chunk_steps: 50
  # training rollout mbs
  num_group_envs: 32
  rollout_epoch: 1

  reward_type: action_level
  logprob_type: action_level
  entropy_type: action_level

  # mbs to do log prob inference
  logprob_forward_micro_batch_size: 16

  # SAC-specific algorithm settings
  adv_type: embodied_sac  # Use SAC returns computation
  loss_type: embodied_sac  # Use SAC loss functions with Gumbel-Softmax
  loss_agg_func: "token-mean"

  bootstrap_type: standard  # r: always -> standard
  
  # Gumbel-Softmax parameters
  use_gumbel_softmax: True  # Enable Gumbel-Softmax for differentiable discrete actions
  gumbel_temperature: 1.0   # Temperature for Gumbel-Softmax sampling
  
  # SAC hyperparameters
  gamma: 0.96
  tau: 0.005  # Soft update coefficient for target networks
  # alpha: 0.2  # Temperature parameter (if not using auto-tuning)
  auto_entropy_tuning: True  # Enable automatic entropy tuning
  alpha_type: softplus
  initial_alpha: 0.01  # Initial temperature value    # r4: 0.2 -> 0.01
  target_entropy: -3  # Target entropy (-action_dim)  # r5: -4 -> -3
  alpha_lr: 3.0e-4  # Learning rate for temperature parameter

  critic_actor_ratio: 4 # r6: 1 -> 4  
  # Replay buffer settings
  replay_buffer_capacity: 20000  # r8: 300000 -> 20000
  min_buffer_size: 200  # Minimum buffer size before training starts
  num_updates_per_step: 64  # Number of gradient updates per training step
  target_update_freq: 1  # Frequency of target network updates

  # params for rollout
  sampling_params:
    do_sample: True
    # use_greedy: False
    temperature_train: 1.0
    temperature_eval: 0.6
    top_k: 50
    top_p: 1.0
    repetition_penalty: 1.0

  # length argument for autoregressive sampling
  length_params:
    max_new_token: 7
    max_length: 1024
    min_length: 1

env:
  group_name: "EnvGroup"
  # channel:  # r9: deleted
  #   name: "env_buffer_list"
  #   queue_name: "obs_buffer"
  #   queue_size: 0
  enable_offload: False

  ## env.train & env.eval, Override config/env/maniskill_pick_cube.yaml
  train:
    ignore_terminations: False  # r10: True -> False
    total_num_envs: 1    # r11: 32 -> 1
    override_cfg:
      # is_dummy: True # [shengyz临时修改12.29]
      # camera_serials: ["0000", ]
      use_dense_reward: False
      # target_ee_pose: [ 0.41941906,  0.28883845,  0.09072751, -3.14, -0.0, 0.41] # 和NUC上env里写死的一致 # 应对应相机朝人
      # [ 0.41579175  0.28357548  0.09172824 -3.14087718 -0.02510057  0.42143215] # 相机朝后
      # target_ee_pose: [4.14090120e-01 , 2.85933788e-01 , 9.16617132e-02 ,-3.13813420e+00 ,-2.60907004e-03 ,-2.69436444e+00] # 相机朝人
      # [ 0.41343721  0.28894925  0.09237983 -3.13208132  0.02861022  2.57261437] # 相机朝人-再微朝左
      # [0.41618152 0.28607053 0.09178186 3.11584795 0.01638882 1.58955859] # 相机朝左
      # target_ee_pose: [ 0.4147261  , 0.28656645 , 0.0916012 ,  3.13545285, -0.01368772, -2.66266673] # 换黄色线后新获取 12-30-21-35
      # target_ee_pose: [ 0.41548371 , 0.28824517 , 0.09228883 , -3.12852531 , 0.01178752 , -2.68800601] # 失败一次后再获取 12-30-22-25
      # target_ee_pose: [ 0.41461746 , 0.28587428 , 0.09155119 , 3.12770918 , 0.01774508 , -2.68052265] # 12-31-00-09
      # target_ee_pose: [ 0.41386361 , 0.28722714 , 0.09176794 , 3.13921051 , 0.00839917 , -2.64822138] # 12-31-10
      # target_ee_pose: [ 0.41481496,  0.28566867,  0.09166707,  3.14085878, -0.00857106,  0.48939543] # 1231-1458
      # target_ee_pose: [0.4148258,  0.28594002, 0.09284807, 3.13906755, 0.00436782, 0.49189969] # 1231-22 新相机架
      # target_ee_pose: [ 0.41330635,  0.28391217,  0.09154761, -3.13590897, -0.01810676,  0.45635886] # [shengyz修改] 0103
      # target_ee_pose: [ 0.41698629,  0.28652657,  0.09155042, -3.13624991, -0.02305444,  0.46411813] # 0104-17 # 测试最新commit
      # target_ee_pose: [ 4.15604618e-01,  2.84799782e-01,  9.03269719e-02, -3.12346426e+00, -3.97273384e-04,  4.38571845e-01] # 0105 测试双D405
      # target_ee_pose: [ 0.41663761, 0.29212138, 0.05575967, 3.13819159, -0.00879289, 0.42918636] # 0106 换board和peg，再测试双D405 (机械臂抽风后恢复)
      target_ee_pose: [ 0.41550194,  0.29121122,  0.0579681,   3.13840229, -0.02567941,  0.48478801] # 0107 最普通单相机能看到peg太少，改俯视支架
    # r12: USE realworld_peg_insertion.yaml, NO OVERRIDE
    # auto_reset: True       
    # use_fixed_reset_state_ids: False
    # max_episode_steps: 50 # max episode steps for truncation
    # max_steps_per_rollout_epoch: 2
    # init_params:
    #   obs_mode: rgb # Must specify obs_mode! 
    #   sensor_configs:
    #     width: 64
    #     height: 64  
  # eval:
  #   total_num_envs: 16
  #   auto_reset: True
  #   ignore_terminations: True
  #   max_episode_steps: 50
  #   max_steps_per_rollout_epoch: 50
  #   use_fixed_reset_state_ids: True
  #   group_size: 1
  #   video_cfg:
  #     save_video: True
  #     video_base_dir: ${runner.logger.log_path}/video/eval
    
  #   init_params:
  #     obs_mode: rgb
  #     sensor_configs:
  #       width: 64
  #       height: 64


rollout:
  group_name: "RolloutGroup"
  backend: "huggingface"
  model_dir: ""
  enable_offload: False
  pipeline_stage_num: 1

  # r: deleted
  # channel:
  #   name: ${env.channel.name}
  #   queue_name: "action_buffer"
  #   queue_size: 0
  # mode: "colocate"  
  
  # gpu_memory_utilization: 0.5
  # enforce_eager: True 

  model:
    model_path: "/home/franka/hongzhi/RLinf/"
    precision: ${actor.model.precision}
    num_q_heads: 10 # r: added

actor:
  group_name: "ActorGroup"
  # r: deleted
  # channel:
  #   name: ${env.channel.name}
  #   queue_name: "replay_buffer"
  #   queue_size: 0
  training_backend: "fsdp"

  # r: deleted
  # checkpoint_load_path: ""
  # checkpoint_save_path: "../results"

  micro_batch_size: 1024  # Smaller batch size for SAC # r: 512 -> 1024
  global_batch_size: 1024   # Smaller global batch size # r: 512 -> 1024
  seed: 1234
  enable_offload: False

  ## actor.model will Override config/model/flow_policy.yaml
  model:
    # model_name: "flow"  # Use flow matching model
    model_type: "flow_policy"
    input_type: "mixed" # ['state', 'mixed'] # IMPORTANT: newly added param by shengyz, to distinguish FlowStatePolicy and FlowPolicy in get_model()
    
    # When use pretrained ckpt, must define actor.model.model_path!
    # In policy, will have: ckpt_path = os.path.join(self.model_path, self.encoder_config["ckpt_name"])
    model_path: "/home/franka/hongzhi/RLinf/"

    # policy_setup: "panda-ee-dpos"
    # image_keys: ["base_camera", ] # x1. image_keys replaced with image_num
    state_dim: 19 # r: 29 -> 19
    action_dim: 6 # r: 4 -> 6
    image_num: 1
    # image_size: [3, 64, 64]
        
    num_action_chunks: 1
    hidden_dim: 256
    
    precision: "32"
    add_value_head: False
    add_q_head: True
    backbone: resnet # ["resnet", "plane_conv"]
    
    # Flow Matching specific parameters
    denoising_steps: 4  # Number of denoising steps for flow matching
    d_model: 96  # Transformer model dimension
    n_head: 4  # Number of attention heads
    n_layers: 2  # Number of transformer layers
    use_batch_norm: False  # Whether to use batch normalization
    batch_norm_momentum: 0.99  # Batch normalization momentum
    flow_actor_type: "JaxFlowTActor"  # "FlowTActor" or "JaxFlowTActor"
    
    is_lora: False
    lora_rank: 32
    ckpt_path: null
    gradient_checkpointing: False
    sharding_strategy: no_shard
    vh_mode: "q_network"
    q_network_separate_processing: True

    # 1. Changes of ResNetEncoder:
    encoder_config: # 'extra_config' rename to 'encoder_config'. Really used in FlowPolicy.
      ckpt_name: "resnet10_pretrained.pt" # 'pretrained_ckpt_path' renamed to 'ckpt_name'

  optim:
    lr: 3.0e-4  # Lower learning rate for SAC
    value_lr: 3.0e-4  # Q-network learning rate
    adam_beta1: 0.9
    adam_beta2: 0.999
    adam_eps: 1.0e-08
    clip_grad: 1.0

  fsdp_config:
    # use_orig_params: False
    strategy: "fsdp"
    sharding_strategy: "no_shard"
    mixed_precision:
      param_dtype: ${actor.model.precision}
      reduce_dtype: ${actor.model.precision}
      buffer_dtype: ${actor.model.precision}

reward:
  use_reward_model: False

critic:
  use_critic_model: False

